{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e46aad",
   "metadata": {},
   "source": [
    "<span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Daily Feature Pipeline for Air Quality (aqicn.org) and weather (openmeteo)</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "1. Download and Parse Data\n",
    "2. Feature Group Insertion\n",
    "\n",
    "\n",
    "__This notebook should be scheduled to run daily__\n",
    "\n",
    "In the book, we use a GitHub Action stored here:\n",
    "[.github/workflows/air-quality-daily.yml](https://github.com/featurestorebook/mlfs-book/blob/main/.github/workflows/air-quality-daily.yml)\n",
    "\n",
    "However, you are free to use any Python Orchestration tool to schedule this program to run daily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe638c6",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de2e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "from functions import util\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import holidays\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6081d1",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üåç Get the Sensor URL, Country, City, Street names from Hopsworks </span>\n",
    "\n",
    "__Update the values in the cell below.__\n",
    "\n",
    "__These should be the same values as in notebook 1 - the feature backfill notebook__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70cd57d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-03 12:46:40,842 INFO: Initializing external client\n",
      "2025-01-03 12:46:40,842 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-01-03 12:46:42,161 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1164440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"city\": \"Barcelona\", \"station_id\": 42, \"bikes_url\": \"https://opendata-ajuntament.barcelona.cat/data/dataset/estat-estacions-bicing/resource/1b215493-9e63-4a12-8980-2d7e0fa19f85/download/recurs.json\", \"latitude\": \"41.404511\", \"longitude\": \"2.189881\"}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you haven't set the env variable 'HOPSWORKS_API_KEY', then uncomment the next line and enter your API key\n",
    "# os.environ[\"HOPSWORKS_API_KEY\"] = \"\"\n",
    "#check if os.environ[\"HOPSWORKS_API_KEY\"] is set\n",
    "if not os.environ.get(\"HOPSWORKS_API_KEY\"):\n",
    "    with open('../../data/hopsworks-api-key.txt', 'r') as file:\n",
    "        os.environ[\"HOPSWORKS_API_KEY\"] = file.read().rstrip()\n",
    "\n",
    "project = hopsworks.login(project=\"juls_first_project\")\n",
    "fs = project.get_feature_store() \n",
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "\n",
    "BICING_API_KEY = secrets.get_secret(\"BICING_API_KEY\").value\n",
    "station_str = secrets.get_secret(\"STATION_PARAMS_JSON\").value\n",
    "station= json.loads(station_str)\n",
    "\n",
    "city=station['city']\n",
    "station_id=station['station_id']\n",
    "bikes_url=station['bikes_url']\n",
    "latitude=station['latitude']\n",
    "longitude=station['longitude']\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "station_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf9289",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> üîÆ Get references to the Feature Groups </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f5d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature groups\n",
    "bikes_fg = fs.get_feature_group(\n",
    "    name='bikes',\n",
    "    version=1,\n",
    ")\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b6ce8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ffa41",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üå´ Retrieve Today's Air Quality data (PM2.5) from the AQI API</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f681af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>num_bikes_available</th>\n",
       "      <th>last_reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>2025-01-03 11:49:49+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id  num_bikes_available             last_reported\n",
       "0          42                   17 2025-01-03 11:49:49+00:00"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "bike_today_df = util.fetch_station_data(bikes_url,BICING_API_KEY, station_id)\n",
    "\n",
    "\n",
    "bike_today_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4144a4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>num_bikes_available</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-03 11:49:49+00:00</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  num_bikes_available         day time  is_weekend  \\\n",
       "0 2025-01-03 11:49:49+00:00                 17.0  2025-01-03   11       False   \n",
       "\n",
       "   is_holiday  \n",
       "0       False  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast last_reported to a string\n",
    "bike_today_df = bike_today_df[['last_reported', 'num_bikes_available']]\n",
    "\n",
    "# Create column 'day' with the date\n",
    "bike_today_df['day'] = bike_today_df['last_reported'].dt.strftime('%Y-%m-%d')\n",
    "# Create column 'time' with the hour\n",
    "bike_today_df['time'] = bike_today_df['last_reported'].dt.strftime('%H')\n",
    "bike_today_df = bike_today_df.rename(columns={\"last_reported\": \"date\"})\n",
    "## Adding a new boolean column if the date is weekend or not\n",
    "bike_today_df['is_weekend'] = bike_today_df['date'].dt.dayofweek > 4\n",
    "## Adding a new boolean column if the date is holiday or not\n",
    "holidays_es = holidays.Spain()\n",
    "bike_today_df['is_holiday'] = bike_today_df['date'].dt.date.astype(str).map(lambda x: x in holidays_es)\n",
    "\n",
    "# cast num_bikes_available to double\n",
    "bike_today_df['num_bikes_available'] = bike_today_df['num_bikes_available'].astype(float)\n",
    "\n",
    "\n",
    "bike_today_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e24eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count  Dtype              \n",
      "---  ------               --------------  -----              \n",
      " 0   date                 1 non-null      datetime64[ns, UTC]\n",
      " 1   num_bikes_available  1 non-null      int64              \n",
      " 2   day                  1 non-null      object             \n",
      " 3   time                 1 non-null      object             \n",
      " 4   is_weekend           1 non-null      bool               \n",
      " 5   is_holiday           1 non-null      bool               \n",
      "dtypes: bool(2), datetime64[ns, UTC](1), int64(1), object(2)\n",
      "memory usage: 166.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "bike_today_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af845ab6",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üå¶ Get Weather Forecast data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2ecb3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates 41.5¬∞N 2.25¬∞E\n",
      "Elevation 13.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temperature</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-03 00:00:00</td>\n",
       "      <td>5.55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-03 01:00:00</td>\n",
       "      <td>5.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-03 02:00:00</td>\n",
       "      <td>6.55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-03 03:00:00</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-03 04:00:00</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2025-01-12 19:00:00</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2025-01-12 20:00:00</td>\n",
       "      <td>5.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2025-01-12 21:00:00</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2025-01-12 22:00:00</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2025-01-12 23:00:00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date  temperature  precipitation\n",
       "0   2025-01-03 00:00:00         5.55            0.0\n",
       "1   2025-01-03 01:00:00         5.95            0.0\n",
       "2   2025-01-03 02:00:00         6.55            0.0\n",
       "3   2025-01-03 03:00:00         6.85            0.0\n",
       "4   2025-01-03 04:00:00         6.50            0.0\n",
       "..                  ...          ...            ...\n",
       "235 2025-01-12 19:00:00         6.30            0.0\n",
       "236 2025-01-12 20:00:00         5.60            0.0\n",
       "237 2025-01-12 21:00:00         4.95            0.0\n",
       "238 2025-01-12 22:00:00         4.25            0.0\n",
       "239 2025-01-12 23:00:00         3.60            0.0\n",
       "\n",
       "[240 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_df = util.get_hourly_weather_forecast(city, latitude, longitude)\n",
    "\n",
    "hourly_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c563109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 240 entries, 0 to 239\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   date           240 non-null    datetime64[ns]\n",
      " 1   temperature    240 non-null    float32       \n",
      " 2   precipitation  240 non-null    float32       \n",
      "dtypes: datetime64[ns](1), float32(2)\n",
      "memory usage: 3.9 KB\n"
     ]
    }
   ],
   "source": [
    "hourly_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f5008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#ff5f27;\">‚¨ÜÔ∏è Uploading new data to the Feature Store</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a9de5ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Producer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Insert new data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbikes_fg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbike_today_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bikes/lib/python3.11/site-packages/hsfs/feature_group.py:2940\u001b[0m, in \u001b[0;36mFeatureGroup.insert\u001b[0;34m(self, features, overwrite, operation, storage, write_options, validation_options, wait)\u001b[0m\n\u001b[1;32m   2937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2938\u001b[0m     write_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffline_backfill_every_hr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr\n\u001b[0;32m-> 2940\u001b[0m job, ge_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_group_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2941\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2944\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_report\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2948\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mget_type()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m   2951\u001b[0m     \u001b[38;5;66;03m# Also, only compute statistics if stream is False.\u001b[39;00m\n\u001b[1;32m   2952\u001b[0m     \u001b[38;5;66;03m# if True, the backfill job has not been triggered and the data has not been inserted (it's in Kafka)\u001b[39;00m\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_statistics()\n",
      "File \u001b[0;32m~/.conda/envs/bikes/lib/python3.11/site-packages/hsfs/core/feature_group_engine.py:212\u001b[0m, in \u001b[0;36mFeatureGroupEngine.insert\u001b[0;34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overwrite:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_group_api\u001b[38;5;241m.\u001b[39mdelete_content(feature_group)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 212\u001b[0m     \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbulk_insert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43monline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    221\u001b[0m     ge_report,\n\u001b[1;32m    222\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/bikes/lib/python3.11/site-packages/hsfs/engine/python.py:815\u001b[0m, in \u001b[0;36mEngine.save_dataframe\u001b[0;34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_dataframe\u001b[39m(\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    802\u001b[0m     feature_group: FeatureGroup,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    809\u001b[0m     validation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    810\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[job\u001b[38;5;241m.\u001b[39mJob]:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;28mhasattr\u001b[39m(feature_group, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXTERNAL_FEATURE_GROUP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39monline_enabled\n\u001b[1;32m    814\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m--> 815\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_dataframe_kafka\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m         \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy_save_dataframe(\n\u001b[1;32m    821\u001b[0m             feature_group,\n\u001b[1;32m    822\u001b[0m             dataframe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m             validation_id,\n\u001b[1;32m    829\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/bikes/lib/python3.11/site-packages/hsfs/engine/python.py:1448\u001b[0m, in \u001b[0;36mEngine._write_dataframe_kafka\u001b[0;34m(self, feature_group, dataframe, offline_write_options)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write_dataframe_kafka\u001b[39m(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1443\u001b[0m     feature_group: Union[FeatureGroup, ExternalFeatureGroup],\n\u001b[1;32m   1444\u001b[0m     dataframe: Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, pl\u001b[38;5;241m.\u001b[39mDataFrame],\n\u001b[1;32m   1445\u001b[0m     offline_write_options: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   1446\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[job\u001b[38;5;241m.\u001b[39mJob]:\n\u001b[1;32m   1447\u001b[0m     initial_check_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m     producer, headers, feature_writers, writer \u001b[38;5;241m=\u001b[39m \u001b[43mkafka_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_kafka_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_multi_part_insert:\n\u001b[1;32m   1454\u001b[0m         \u001b[38;5;66;03m# set initial_check_point to the current offset\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m         initial_check_point \u001b[38;5;241m=\u001b[39m kafka_engine\u001b[38;5;241m.\u001b[39mkafka_get_offsets(\n\u001b[1;32m   1456\u001b[0m             topic_name\u001b[38;5;241m=\u001b[39mfeature_group\u001b[38;5;241m.\u001b[39m_online_topic_name,\n\u001b[1;32m   1457\u001b[0m             feature_store_id\u001b[38;5;241m=\u001b[39mfeature_group\u001b[38;5;241m.\u001b[39mfeature_store_id,\n\u001b[1;32m   1458\u001b[0m             offline_write_options\u001b[38;5;241m=\u001b[39moffline_write_options,\n\u001b[1;32m   1459\u001b[0m             high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1460\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/bikes/lib/python3.11/site-packages/hsfs/core/kafka_engine.py:76\u001b[0m, in \u001b[0;36minit_kafka_resources\u001b[0;34m(feature_group, offline_write_options, project_id)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_multi_part_insert \u001b[38;5;129;01mand\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_kafka_producer:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     71\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_kafka_producer,\n\u001b[1;32m     72\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_kafka_headers,\n\u001b[1;32m     73\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_feature_writers,\n\u001b[1;32m     74\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_writer,\n\u001b[1;32m     75\u001b[0m     )\n\u001b[0;32m---> 76\u001b[0m producer, headers, feature_writers, writer \u001b[38;5;241m=\u001b[39m \u001b[43m_init_kafka_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_id\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_multi_part_insert:\n\u001b[1;32m     80\u001b[0m     feature_group\u001b[38;5;241m.\u001b[39m_kafka_producer \u001b[38;5;241m=\u001b[39m producer\n",
      "File \u001b[0;32m~/.conda/envs/bikes/lib/python3.11/site-packages/hsfs/core/kafka_engine.py:95\u001b[0m, in \u001b[0;36m_init_kafka_resources\u001b[0;34m(feature_group, offline_write_options, project_id)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_kafka_resources\u001b[39m(\n\u001b[1;32m     88\u001b[0m     feature_group: Union[FeatureGroup, ExternalFeatureGroup],\n\u001b[1;32m     89\u001b[0m     offline_write_options: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m ]:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# setup kafka producer\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     producer \u001b[38;5;241m=\u001b[39m \u001b[43minit_kafka_producer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_store_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# setup complex feature writers\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     feature_writers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    100\u001b[0m         feature: get_encoder_func(feature_group\u001b[38;5;241m.\u001b[39m_get_feature_avro_schema(feature))\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39mget_complex_features()\n\u001b[1;32m    102\u001b[0m     }\n",
      "File \u001b[0;32m~/.conda/envs/bikes/lib/python3.11/site-packages/hsfs/core/kafka_engine.py:120\u001b[0m, in \u001b[0;36minit_kafka_producer\u001b[0;34m(feature_store_id, offline_write_options)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_kafka_producer\u001b[39m(\n\u001b[1;32m    116\u001b[0m     feature_store_id: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    117\u001b[0m     offline_write_options: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    118\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Producer:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# setup kafka producer\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mProducer\u001b[49m(get_kafka_config(feature_store_id, offline_write_options))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Producer' is not defined"
     ]
    }
   ],
   "source": [
    "# Insert new data\n",
    "bikes_fg.insert(bike_today_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d491b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 14:48:07,861 INFO: \t2 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/398/fs/335/fg/1340773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4023b9d57f6f40df8c98478bc28d1117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/10 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/398/jobs/named/weather_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7f5f18354cd0>,\n",
       " {\n",
       "   \"success\": true,\n",
       "   \"results\": [\n",
       "     {\n",
       "       \"success\": true,\n",
       "       \"expectation_config\": {\n",
       "         \"expectation_type\": \"expect_column_min_to_be_between\",\n",
       "         \"kwargs\": {\n",
       "           \"column\": \"precipitation_sum\",\n",
       "           \"min_value\": -0.1,\n",
       "           \"max_value\": 1000.0,\n",
       "           \"strict_min\": true\n",
       "         },\n",
       "         \"meta\": {\n",
       "           \"expectationId\": 664596\n",
       "         }\n",
       "       },\n",
       "       \"result\": {\n",
       "         \"observed_value\": 0.0,\n",
       "         \"element_count\": 10,\n",
       "         \"missing_count\": null,\n",
       "         \"missing_percent\": null\n",
       "       },\n",
       "       \"meta\": {\n",
       "         \"ingestionResult\": \"INGESTED\",\n",
       "         \"validationTime\": \"2024-11-07T01:48:07.000860Z\"\n",
       "       },\n",
       "       \"exception_info\": {\n",
       "         \"raised_exception\": false,\n",
       "         \"exception_message\": null,\n",
       "         \"exception_traceback\": null\n",
       "       }\n",
       "     },\n",
       "     {\n",
       "       \"success\": true,\n",
       "       \"expectation_config\": {\n",
       "         \"expectation_type\": \"expect_column_min_to_be_between\",\n",
       "         \"kwargs\": {\n",
       "           \"column\": \"wind_speed_10m_max\",\n",
       "           \"min_value\": -0.1,\n",
       "           \"max_value\": 1000.0,\n",
       "           \"strict_min\": true\n",
       "         },\n",
       "         \"meta\": {\n",
       "           \"expectationId\": 664597\n",
       "         }\n",
       "       },\n",
       "       \"result\": {\n",
       "         \"observed_value\": 9.793058395385742,\n",
       "         \"element_count\": 10,\n",
       "         \"missing_count\": null,\n",
       "         \"missing_percent\": null\n",
       "       },\n",
       "       \"meta\": {\n",
       "         \"ingestionResult\": \"INGESTED\",\n",
       "         \"validationTime\": \"2024-11-07T01:48:07.000860Z\"\n",
       "       },\n",
       "       \"exception_info\": {\n",
       "         \"raised_exception\": false,\n",
       "         \"exception_message\": null,\n",
       "         \"exception_traceback\": null\n",
       "       }\n",
       "     }\n",
       "   ],\n",
       "   \"evaluation_parameters\": {},\n",
       "   \"statistics\": {\n",
       "     \"evaluated_expectations\": 2,\n",
       "     \"successful_expectations\": 2,\n",
       "     \"unsuccessful_expectations\": 0,\n",
       "     \"success_percent\": 100.0\n",
       "   },\n",
       "   \"meta\": {\n",
       "     \"great_expectations_version\": \"0.18.12\",\n",
       "     \"expectation_suite_name\": \"weather_expectation_suite\",\n",
       "     \"run_id\": {\n",
       "       \"run_name\": null,\n",
       "       \"run_time\": \"2024-11-07T14:48:07.861128+01:00\"\n",
       "     },\n",
       "     \"batch_kwargs\": {\n",
       "       \"ge_batch_id\": \"eb899f1c-9d0e-11ef-994e-00155dab386a\"\n",
       "     },\n",
       "     \"batch_markers\": {},\n",
       "     \"batch_parameters\": {},\n",
       "     \"validation_time\": \"20241107T134807.860869Z\",\n",
       "     \"expectation_suite_meta\": {\n",
       "       \"great_expectations_version\": \"0.18.12\"\n",
       "     }\n",
       "   }\n",
       " })"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert new data\n",
    "weather_fg.insert(daily_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e9e2d",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 03: Training Pipeline\n",
    " </span> \n",
    "\n",
    "In the following notebook you will read from a feature group and create training dataset within the feature store\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bikes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
